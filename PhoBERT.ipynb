{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the original .txt files\n",
    "input_folder = r\"C:\\Users\\ADMIN\\Documents\\Document\\USTH\\B3\\nlp\\legaldoc_summarize\\under2k_chunked\"\n",
    "\n",
    "# Output folder where the split chunks will be saved\n",
    "output_folder = r\"C:\\Users\\ADMIN\\Documents\\Document\\USTH\\B3\\nlp\\split_chunks\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all .txt files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Split content by chunks, using \"### Chunk X ###\" as the marker\n",
    "        chunks = re.split(r'(?=### Chunk\\s*\\d+\\s*###)', content)\n",
    "        chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "        \n",
    "        base_name = os.path.splitext(filename)[0]  # e.g., \"187\" from \"187.txt\"\n",
    "\n",
    "        # Save each chunk to a new file\n",
    "        for index, chunk in enumerate(chunks, start=1):\n",
    "            output_filename = f\"{base_name}_{index}.txt\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(chunk)\n",
    "\n",
    "print(\"✅ All chunks have been successfully split and saved to 'split_chunks' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained Vietnamese summarization model\n",
    "model_name = \"VietAI/vit5-base-vietnews-summarization\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_sentences=2):\n",
    "    # Remove headers like \"### Chunk 1 ###\"\n",
    "    text = re.sub(r\"### Chunk\\s*\\d+\\s*###\", \"\", text).strip()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        min_length=30,\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return only the first 2 sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', summary)\n",
    "    return \" \".join(sentences[:max_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all .txt files and summarize them\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Generate summary\n",
    "        summary = summarize_text(text)\n",
    "\n",
    "        # Construct output filename, e.g., 187_1_summarize.txt\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        output_filename = base_name + \"_summarize.txt\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # Save summary to file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            out_f.write(summary)\n",
    "\n",
    "        print(f\"✅ Summarized: {filename} → {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
